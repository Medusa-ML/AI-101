{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_output/fandom'\n",
    "\n",
    "embedding_dim = 64\n",
    "max_features=10000\n",
    "maxlen=300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elfen-lied.fandom.com</td>\n",
       "      <td>[elfen, lied, wikiwelcome, currently, maintain...</td>\n",
       "      <td>{'3rdpersonshooter': 0, 'abc': 0, 'action': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elfen-lied.fandom.com</td>\n",
       "      <td>[including, kurama, secretary, read, featured,...</td>\n",
       "      <td>{'3rdpersonshooter': 0, 'abc': 0, 'action': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>warhammer40kfanon.fandom.com</td>\n",
       "      <td>[warhammer, wikidisclaimer, adeptus, astartes,...</td>\n",
       "      <td>{'3rdpersonshooter': 0, 'abc': 0, 'action': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayakashi-ghost-guild.fandom.com</td>\n",
       "      <td>[ayakashi, ghost, guild, onmyouroku, wikilates...</td>\n",
       "      <td>{'3rdpersonshooter': 0, 'abc': 0, 'action': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utaite.fandom.com</td>\n",
       "      <td>[utaite, wikiplease, read, start, editing, wel...</td>\n",
       "      <td>{'3rdpersonshooter': 0, 'abc': 0, 'action': 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               url  \\\n",
       "0            elfen-lied.fandom.com   \n",
       "1            elfen-lied.fandom.com   \n",
       "2     warhammer40kfanon.fandom.com   \n",
       "3  ayakashi-ghost-guild.fandom.com   \n",
       "4                utaite.fandom.com   \n",
       "\n",
       "                                                text  \\\n",
       "0  [elfen, lied, wikiwelcome, currently, maintain...   \n",
       "1  [including, kurama, secretary, read, featured,...   \n",
       "2  [warhammer, wikidisclaimer, adeptus, astartes,...   \n",
       "3  [ayakashi, ghost, guild, onmyouroku, wikilates...   \n",
       "4  [utaite, wikiplease, read, start, editing, wel...   \n",
       "\n",
       "                                                tags  \n",
       "0  {'3rdpersonshooter': 0, 'abc': 0, 'action': 1,...  \n",
       "1  {'3rdpersonshooter': 0, 'abc': 0, 'action': 1,...  \n",
       "2  {'3rdpersonshooter': 0, 'abc': 0, 'action': 0,...  \n",
       "3  {'3rdpersonshooter': 0, 'abc': 0, 'action': 0,...  \n",
       "4  {'3rdpersonshooter': 0, 'abc': 0, 'action': 0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the parquet data frame... nothing that special here\n",
    "\n",
    "filepath = os.path.abspath(os.path.join(os.getcwd(), \n",
    "        \"..\", \n",
    "        \"data/fandom_categorizer/downloads/scraped/master_8-26_expanddata_3468.parquet\"))\n",
    "\n",
    "raw_df = pd.read_parquet(filepath)\n",
    "    \n",
    "#append to dataframe\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Labels (IAB Categories, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3965\n"
     ]
    }
   ],
   "source": [
    "y = [list(v.values()) for k,v in raw_df.tags.iteritems()]\n",
    "print(y[0]) # our total list of labels\n",
    "print(len(y)) # the total number of labeled sites\n",
    "num_categories = len(y[0]) # the number of labels/IAB categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup \"Word Bags\" (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tf.keras.preprocessing.text.Tokenizer(num_words=max_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.fit_on_texts(list(raw_df.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78446\n"
     ]
    }
   ],
   "source": [
    "print(len(tok.word_index))\n",
    "with io.open(output_dir+'/tokenizer.json','w',encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tok.to_json(),ensure_ascii=False))\n",
    "vocab_size = len(tok.word_index) + 1 \n",
    "#this represents the number of words that we tokenize different from max_features but necessary for\n",
    "#the definition of the dimension of the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0, 4524, 4013,  267,   60, 1510,   12,\n",
       "        199,  103,  212,   11,    1,   91,  155,  236,  139, 1278,   76,\n",
       "        139,  247, 5336,   43, 4524, 4013,    9, 4624,  338,  146,   58,\n",
       "         40,  115,   11, 3070,   15,  222,  392,  131,  602, 3848,    6,\n",
       "        262,    6, 3441,  199,  108,   11,   11,  336, 2299, 3940,   14,\n",
       "       4524, 4013,    1,    6,   13,    2,    3, 4524, 4013,  703, 7906,\n",
       "       7036, 9059, 7037, 4525,  810,  536,  536,   65,  944, 4526,  196,\n",
       "       8271,  150,  933,  114,  980,  129,   93, 1068,  322, 2037, 3941,\n",
       "       1873, 3547,    5,  155,  302,  189, 1269,   74, 1343,   74,  223,\n",
       "         74,   45,   74,  139,   74,   82,    3,   74,   17,    3,   74,\n",
       "       1060,  236,   76,  715,  443,  336,  140,    1,  299,   11,    6,\n",
       "       2367,    6,  733, 1812,  262,    6,   19,    1,   35,   26,    3,\n",
       "          5,   20,   16,   51,   23,   75,  249,   32,   30,   22,   17,\n",
       "         31,   18,   60, 1510,   12,  199,  103,  212,   11,    1,   91,\n",
       "        155,  236,  139, 1278,   76,  139,  247, 5336,   43, 4524, 4013,\n",
       "          9, 4624,  338,  146,   58,   40,  115,   11, 3070,   15,  222,\n",
       "        392,  131,  602, 3848,    6,  262,    6, 3441,  199,  108,   11,\n",
       "         11,  336, 2299, 3940,   11,  274,  302,  189, 1269,   74, 1343,\n",
       "         74,    5,  177,    8,  348,  440,  348,  373,  351,   58,  345,\n",
       "         68, 1786, 1194,   40,   53,   29, 1477,  134,  652,  644,  877,\n",
       "         98,   78,  651,    8,    8,   58,   40,    9, 2320, 2184, 2320,\n",
       "       5655,   25, 2974,   23,   81,  518,  153,  657, 4834, 3278,   21,\n",
       "        207,  852,   11,  116,   98, 1699,   43,   99, 3279,    9,   38,\n",
       "         36, 7036,  250, 1983, 2643, 3849,  704, 9060,  357,  944, 4526,\n",
       "       2802, 9529,  178, 2099,  956,    9, 1442, 3942,  915, 1787, 4167,\n",
       "        217, 3331, 3943], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = tok.texts_to_sequences(list(raw_df.text)) #this is how we create sequences\n",
    "train_df = tf.keras.preprocessing.sequence.pad_sequences(train_df, maxlen=maxlen) #let's execute pad step\n",
    "train_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 64)           5020608   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2000)              38402000  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 61)                6161      \n",
      "=================================================================\n",
      "Total params: 44,479,369\n",
      "Trainable params: 44,479,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embedding_dim,\n",
    "                           input_length=maxlen),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(2000,activation='relu'),\n",
    "  tf.keras.layers.Dense(500,activation='relu'),\n",
    "  tf.keras.layers.Dense(100,activation='relu'),\n",
    "  tf.keras.layers.Dense(num_categories, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='nadam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=output_dir+\n",
    "                                  \"/weights.{epoch:02d}.hdf5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train),\n",
    "          #batch_size=128,\n",
    "          validation_data=(np.array(X_test),np.array(y_test)),\n",
    "          epochs=20,\n",
    "          callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(np.array(X_test), np.array(y_test)) \n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.10.hdf5\") #NOT ZERO INDEXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   3rdpersonshooter   abc  action  adventure    ae  amazon  \\\n",
      "Correct                        2640  3392    2132       1867  3203    3448   \n",
      "Wrong-OverTagged                459    77    1430        976   186      45   \n",
      "Wrong-UnderTagged               469    99       6        725   179      75   \n",
      "Error                             0     0       0          0     0       0   \n",
      "Count                          3568  3568    3568       3568  3568    3568   \n",
      "\n",
      "                    amc  anime  bbcamerica  books  ...  showtime   sim  \\\n",
      "Correct            3459   1806        3477   2098  ...      3492  2914   \n",
      "Wrong-OverTagged     48    877          38    667  ...        35   291   \n",
      "Wrong-UnderTagged    61    885          53    803  ...        41   363   \n",
      "Error                 0      0           0      0  ...         0     0   \n",
      "Count              3568   3568        3568   3568  ...      3568  3568   \n",
      "\n",
      "                   sports  strategy  syfy  thriller    tv   war   web  western  \n",
      "Correct              3248      2680  3484      2870  2098  2893  2869     3470  \n",
      "Wrong-OverTagged      145       389    42       339  1470   318   331       52  \n",
      "Wrong-UnderTagged     175       499    42       359     0   357   368       46  \n",
      "Error                   0         0     0         0     0     0     0        0  \n",
      "Count                3568      3568  3568      3568  3568  3568  3568     3568  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.abspath(os.path.join(os.getcwd(), \n",
    "            \"..\",\n",
    "            \"data/fandom_categorizer/fandom_tags.json\"))\n",
    "with open(filepath) as f:\n",
    "    label_dict = json.load(f)\n",
    "\n",
    "results = {}\n",
    "    \n",
    "for k,v in label_dict.items():\n",
    "    results[k]={\n",
    "        'Correct':0,\n",
    "        'Wrong-OverTagged':0,\n",
    "        'Wrong-UnderTagged':0,\n",
    "        'Error':0,\n",
    "        'Count':0,\n",
    "        }\n",
    "\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    j=0\n",
    "    for k,v in label_dict.items():\n",
    "        p = 1 if predictions[i][j] > 0.90 else 0\n",
    "        a = y[i][j]\n",
    "        if p >= .90 and a == 1 or p == 0 and a == 0:\n",
    "            results[k]['Correct'] += 1\n",
    "        elif p >= .90 and a == 0:\n",
    "            results[k]['Wrong-OverTagged'] += 1\n",
    "        elif p < .90 and a == 1:\n",
    "            results[k]['Wrong-UnderTagged'] += 1\n",
    "        else:\n",
    "            results[k]['Error'] += 1\n",
    "        results[k]['Count'] += 1    \n",
    "        j+=1\n",
    "\n",
    "pred_df = pd.DataFrame(data=results)\n",
    "print(pred_df)\n",
    "pred_df.to_csv(output_dir+\"/label-accuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load/Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(output_dir+'/model_v003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
